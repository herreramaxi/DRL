nohup: ignoring input
[0;37mParsed arguments:[0m
[0;37m   n_samples: 100000[0m
[0;37m   board_file_path: boards/boards.npy[0m
[0;37m   weights_file_path: weights/ae_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   n_samples: 100000[0m
[0;37m   board_file_path: boards/board_seqs.npy[0m
[0;37m   weights_file_path: weights/ae_rnn_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 2_MaskablePPO_Baseline[0m
[0;37m   model_path: models/2_MaskablePPO_Baseline_20250806_013143.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 5000000[0m
[0;37m   n_envs: 8[0m
[0;37m   n_steps: 4096[0m
[0;37m   batch_size: 4096[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
[0;37m   share_features_extractor: True[0m
[0;37m   print_model_only: False[0m
Training '2_MaskablePPO_Baseline' agent using device 'cuda' and '8' parallel environments...
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Active env processes: 8
[0;35m-> share_features_extractor: True[0m
Using cuda device
[0;36m-> Model summary:[0m
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MaskableMultiInputActorCriticPolicy      --
â”œâ”€CombinedExtractor: 1-1                 --
â”‚    â””â”€ModuleDict: 2-1                   --
â”‚    â”‚    â””â”€Flatten: 3-1                 --
â”œâ”€CombinedExtractor: 1-2                 --
â”‚    â””â”€ModuleDict: 2-2                   --
â”‚    â”‚    â””â”€Flatten: 3-2                 --
â”œâ”€CombinedExtractor: 1-3                 --
â”‚    â””â”€ModuleDict: 2-3                   --
â”‚    â”‚    â””â”€Flatten: 3-3                 --
â”œâ”€MlpExtractor: 1-4                      --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-4                  1,664
â”‚    â”‚    â””â”€Tanh: 3-5                    --
â”‚    â”‚    â””â”€Linear: 3-6                  4,160
â”‚    â”‚    â””â”€Tanh: 3-7                    --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-8                  1,664
â”‚    â”‚    â””â”€Tanh: 3-9                    --
â”‚    â”‚    â””â”€Linear: 3-10                 4,160
â”‚    â”‚    â””â”€Tanh: 3-11                   --
â”œâ”€Linear: 1-5                            61,230
â”œâ”€Linear: 1-6                            65
=================================================================
Total params: 72,943
Trainable params: 72,943
Non-trainable params: 0
=================================================================
Logging to ./chess_logs/2_MaskablePPO_Baseline_5
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 1564     |
|    invalid_moves   | 0        |
|    losses          | 767      |
|    valid_moves     | 30000    |
|    win_rate        | 0.51     |
|    wins            | 797      |
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 1633     |
|    iterations      | 1        |
|    time_elapsed    | 20       |
|    total_timesteps | 32768    |
---------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 3433          |
|    invalid_moves        | 0             |
|    losses               | 1632          |
|    valid_moves          | 65000         |
|    win_rate             | 0.525         |
|    wins                 | 1801          |
| rollout/                |               |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 38.7          |
| time/                   |               |
|    fps                  | 1608          |
|    iterations           | 2             |
|    time_elapsed         | 40            |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00043643184 |
|    clip_fraction        | 8.85e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.58         |
|    explained_variance   | 0.00145       |
|    learning_rate        | 0.0001        |
|    loss                 | 693           |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.00389      |
|    value_loss           | 1.47e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 5027          |
|    invalid_moves        | 0             |
|    losses               | 2362          |
|    valid_moves          | 95000         |
|    win_rate             | 0.53          |
|    wins                 | 2665          |
| rollout/                |               |
|    ep_len_mean          | 17.4          |
|    ep_rew_mean          | 6.84          |
| time/                   |               |
|    fps                  | 1608          |
|    iterations           | 3             |
|    time_elapsed         | 61            |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00057911733 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.6          |
|    explained_variance   | 0.0033        |
|    learning_rate        | 0.0001        |
|    loss                 | 771           |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00562      |
|    value_loss           | 1.49e+03      |
-------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 6925         |
|    invalid_moves        | 0            |
|    losses               | 3171         |
|    valid_moves          | 130000       |
|    win_rate             | 0.542        |
|    wins                 | 3754         |
| rollout/                |              |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 38.9         |
| time/                   |              |
|    fps                  | 1606         |
|    iterations           | 4            |
|    time_elapsed         | 81           |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0009698792 |
|    clip_fraction        | 0.00139      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.00443      |
|    learning_rate        | 0.0001       |
|    loss                 | 774          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0075      |
|    value_loss           | 1.51e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 8556         |
|    invalid_moves        | 0            |
|    losses               | 3802         |
|    valid_moves          | 160000       |
|    win_rate             | 0.556        |
|    wins                 | 4754         |
| rollout/                |              |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | 27.6         |
| time/                   |              |
|    fps                  | 1605         |
|    iterations           | 5            |
|    time_elapsed         | 102          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.0016845339 |
|    clip_fraction        | 0.00307      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.57        |
|    explained_variance   | 0.00506      |
|    learning_rate        | 0.0001       |
|    loss                 | 740          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00895     |
|    value_loss           | 1.51e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 10610        |
|    invalid_moves        | 0            |
|    losses               | 4509         |
|    valid_moves          | 195000       |
|    win_rate             | 0.575        |
|    wins                 | 6101         |
| rollout/                |              |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 64.4         |
| time/                   |              |
|    fps                  | 1601         |
|    iterations           | 6            |
|    time_elapsed         | 122          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0015289313 |
|    clip_fraction        | 0.00338      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.00607      |
|    learning_rate        | 0.0001       |
|    loss                 | 822          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00991     |
|    value_loss           | 1.54e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 12492        |
|    invalid_moves        | 0            |
|    losses               | 5056         |
|    valid_moves          | 225000       |
|    win_rate             | 0.595        |
|    wins                 | 7436         |
| rollout/                |              |
|    ep_len_mean          | 14           |
|    ep_rew_mean          | 85.1         |
| time/                   |              |
|    fps                  | 1595         |
|    iterations           | 7            |
|    time_elapsed         | 143          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0023076488 |
|    clip_fraction        | 0.00754      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.00859      |
|    learning_rate        | 0.0001       |
|    loss                 | 797          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 1.62e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 14966       |
|    invalid_moves        | 0           |
|    losses               | 5647        |
|    valid_moves          | 260000      |
|    win_rate             | 0.623       |
|    wins                 | 9319        |
| rollout/                |             |
|    ep_len_mean          | 14.5        |
|    ep_rew_mean          | 94.3        |
| time/                   |             |
|    fps                  | 1586        |
|    iterations           | 8           |
|    time_elapsed         | 165         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.002639018 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.0085      |
|    learning_rate        | 0.0001      |
|    loss                 | 838         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Traceback (most recent call last):
  File "/home/herreramaxi/DRL/Chess_2_MaskablePPO.py", line 53, in <module>
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
    model_learn(args, model)
  File "/home/herreramaxi/DRL/common.py", line 101, in model_learn
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
    model.learn(total_timesteps=args.total_timesteps, tb_log_name=args.agent_name,callback=callback)
  File "/home/herreramaxi/DRL/stable-baselines3-contrib/sb3_contrib/ppo_mask/ppo_mask.py", line 454, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, self.n_steps, use_masking)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/DRL/stable-baselines3-contrib/sb3_contrib/ppo_mask/ppo_mask.py", line 230, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor, action_masks=action_masks)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/DRL/stable-baselines3-contrib/sb3_contrib/common/maskable/policies.py", line 139, in forward
    distribution.apply_masking(action_masks)
  File "/home/herreramaxi/DRL/stable-baselines3-contrib/sb3_contrib/common/maskable/distributions.py", line 167, in apply_masking
    self.distribution.apply_masking(masks)
  File "/home/herreramaxi/DRL/stable-baselines3-contrib/sb3_contrib/common/maskable/distributions.py", line 68, in apply_masking
    super().__init__(logits=logits)
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/torch/distributions/categorical.py", line 72, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/torch/distributions/distribution.py", line 70, in __init__
    if not valid.all():
           ^^^^^^^^^^^
KeyboardInterrupt
[0;36m-> Starting experiments...[0m
[0;37mParsed arguments:[0m
[0;37m  parallel: False[0m
[0;37m  max_workers: 3[0m
[0;37m  num_repeats: 3[0m
[0;37m  experiments: ['2', '5', '7', '8'][0m
[0;37mArguments to be forwarded: ['--n-envs', '8', '--total-timesteps', '5000000', '--batch-size', '4096', '--n-steps', '4096', '--n-epochs', '10', '--share-features-extractor'][0m
[0;35m-> Experiments to run: ['Chess_2_MaskablePPO.py', 'Chess_5_FF_Autoencoder_MaskableRecurrentPPO.py', 'Chess_7_LSTM_Autoencoder_MaskableRecurrentPPO.py', 'Chess_8_Transformer.py'][0m
[0;36m-> Running Preprocessing Tasks...[0m
[0;35m-> Running script: ae_pretrain.py with args ['--n-samples', '100000', '--n-epochs', '20', '--force-clean', 'False'][0m
[0;32m-> [ae_pretrain.py] Finished in 2.7s[0m
[0;35m-> Running script: ae_rnn_pretrain.py with args ['--n-samples', '100000', '--n-epochs', '20', '--force-clean', 'False'][0m
[0;32m-> [ae_rnn_pretrain.py] Finished in 2.7s[0m
[0;35m-> Running experiments, num_repeats: 3, mode: Sequential[0m
[0;36m-> Iteration: 1[0m
[0;35m-> Running script: Chess_2_MaskablePPO.py with args ['--n-envs', '8', '--total-timesteps', '5000000', '--batch-size', '4096', '--n-steps', '4096', '--n-epochs', '10', '--share-features-extractor'][0m
Traceback (most recent call last):
  File "/home/herreramaxi/DRL/train.py", line 149, in <module>
    run_experiments(args.num_repeats, args.parallel == "True", experiments)    
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/DRL/train.py", line 85, in run_experiments
    run_exp(exp, unknown)
  File "/home/herreramaxi/DRL/train.py", line 101, in run_exp
    subprocess.run(cmd, check=True)
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 1201, in communicate
    self.wait()
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
