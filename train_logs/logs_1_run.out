nohup: ignoring input
[0;37mParsed arguments:[0m
[0;37m   n_samples: 100000[0m
[0;37m   board_file_path: boards/boards.npy[0m
[0;37m   weights_file_path: weights/ae_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   n_samples: 100000[0m
[0;37m   board_file_path: boards/board_seqs.npy[0m
[0;37m   weights_file_path: weights/ae_rnn_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 1_PPO[0m
[0;37m   model_path: models/1_PPO_20250804_133224.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '1_PPO' agent using device 'cuda' and '16' parallel environments...
Using cuda device
[0;36m-> Model summary:[0m
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MultiInputActorCriticPolicy              --
â”œâ”€CombinedExtractor: 1-1                 --
â”‚    â””â”€ModuleDict: 2-1                   --
â”‚    â”‚    â””â”€Flatten: 3-1                 --
â”‚    â”‚    â””â”€Flatten: 3-2                 --
â”œâ”€CombinedExtractor: 1-2                 --
â”‚    â””â”€ModuleDict: 2-2                   --
â”‚    â”‚    â””â”€Flatten: 3-3                 --
â”‚    â”‚    â””â”€Flatten: 3-4                 --
â”œâ”€CombinedExtractor: 1-3                 --
â”‚    â””â”€ModuleDict: 2-3                   --
â”‚    â”‚    â””â”€Flatten: 3-5                 --
â”‚    â”‚    â””â”€Flatten: 3-6                 --
â”œâ”€MlpExtractor: 1-4                      --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  61,952
â”‚    â”‚    â””â”€Tanh: 3-8                    --
â”‚    â”‚    â””â”€Linear: 3-9                  4,160
â”‚    â”‚    â””â”€Tanh: 3-10                   --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-11                 61,952
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”‚    â”‚    â””â”€Linear: 3-13                 4,160
â”‚    â”‚    â””â”€Tanh: 3-14                   --
â”œâ”€Linear: 1-5                            61,230
â”œâ”€Linear: 1-6                            65
=================================================================
Total params: 193,519
Trainable params: 193,519
Non-trainable params: 0
=================================================================
Logging to ./chess_logs/1_PPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 2        |
|    invalid_moves   | 128914   |
|    losses          | 2        |
|    valid_moves     | 1086     |
|    win_rate        | 0        |
|    wins            | 0        |
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -9.91    |
| time/              |          |
|    fps             | 5929     |
|    iterations      | 1        |
|    time_elapsed    | 22       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 2            |
|    invalid_moves        | 257699       |
|    losses               | 2            |
|    valid_moves          | 2301         |
|    win_rate             | 0            |
|    wins                 | 0            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.99        |
| time/                   |              |
|    fps                  | 5517         |
|    iterations           | 2            |
|    time_elapsed         | 47           |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0037011923 |
|    clip_fraction        | 0.00783      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.85        |
|    explained_variance   | -0.00813     |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0609      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 0.431        |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 4           |
|    invalid_moves        | 386247      |
|    losses               | 3           |
|    valid_moves          | 3753        |
|    win_rate             | 0.25        |
|    wins                 | 1           |
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -9.92       |
| time/                   |             |
|    fps                  | 5414        |
|    iterations           | 3           |
|    time_elapsed         | 72          |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.003288025 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | -16.3       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0822     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 0.00393     |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 7            |
|    invalid_moves        | 514542       |
|    losses               | 6            |
|    valid_moves          | 5458         |
|    win_rate             | 0.143        |
|    wins                 | 1            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.9         |
| time/                   |              |
|    fps                  | 5331         |
|    iterations           | 4            |
|    time_elapsed         | 98           |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0037184877 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | -0.00709     |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0802      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00614     |
|    value_loss           | 0.434        |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 13           |
|    invalid_moves        | 642582       |
|    losses               | 10           |
|    valid_moves          | 7418         |
|    win_rate             | 0.231        |
|    wins                 | 3            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.91        |
| time/                   |              |
|    fps                  | 5251         |
|    iterations           | 5            |
|    time_elapsed         | 124          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0032610744 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | -0.00238     |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0802      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 0.645        |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 17           |
|    invalid_moves        | 770492       |
|    losses               | 13           |
|    valid_moves          | 9508         |
|    win_rate             | 0.235        |
|    wins                 | 4            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.92        |
| time/                   |              |
|    fps                  | 5184         |
|    iterations           | 6            |
|    time_elapsed         | 151          |
|    total_timesteps      | 786432       |
| train/                  |              |
|    approx_kl            | 0.0024582369 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.82        |
|    explained_variance   | -0.00101     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.52         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 1.29         |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 24           |
|    invalid_moves        | 898006       |
|    losses               | 18           |
|    valid_moves          | 11994        |
|    win_rate             | 0.25         |
|    wins                 | 6            |
| rollout/                |              |
|    ep_len_mean          | 99.9         |
|    ep_rew_mean          | -8.19        |
| time/                   |              |
|    fps                  | 5115         |
|    iterations           | 7            |
|    time_elapsed         | 179          |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0034711263 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.81        |
|    explained_variance   | -0.00352     |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0713       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00441     |
|    value_loss           | 0.863        |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 36           |
|    invalid_moves        | 1025147      |
|    losses               | 24           |
|    valid_moves          | 14853        |
|    win_rate             | 0.333        |
|    wins                 | 12           |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.89        |
| time/                   |              |
|    fps                  | 5067         |
|    iterations           | 8            |
|    time_elapsed         | 206          |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0028269587 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | -0.000352    |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 1.73         |
------------------------------------------
[0;32m-> Model saved on models/1_PPO_20250804_133224.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Evaluating 1_PPO agent...
[0;36m-> Mean Reward: -10.00 +/- 0.00[0m
[0;37mParsed arguments:[0m
[0;37m   agent_name: 2_MaskablePPO_Baseline[0m
[0;37m   model_path: models/2_MaskablePPO_Baseline_20250804_133608.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '2_MaskablePPO_Baseline' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MaskableMultiInputActorCriticPolicy      --
â”œâ”€CombinedExtractor: 1-1                 --
â”‚    â””â”€ModuleDict: 2-1                   --
â”‚    â”‚    â””â”€Flatten: 3-1                 --
â”œâ”€CombinedExtractor: 1-2                 --
â”‚    â””â”€ModuleDict: 2-2                   --
â”‚    â”‚    â””â”€Flatten: 3-2                 --
â”œâ”€CombinedExtractor: 1-3                 --
â”‚    â””â”€ModuleDict: 2-3                   --
â”‚    â”‚    â””â”€Flatten: 3-3                 --
â”œâ”€MlpExtractor: 1-4                      --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-4                  1,664
â”‚    â”‚    â””â”€Tanh: 3-5                    --
â”‚    â”‚    â””â”€Linear: 3-6                  4,160
â”‚    â”‚    â””â”€Tanh: 3-7                    --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-8                  1,664
â”‚    â”‚    â””â”€Tanh: 3-9                    --
â”‚    â”‚    â””â”€Linear: 3-10                 4,160
â”‚    â”‚    â””â”€Tanh: 3-11                   --
â”œâ”€Linear: 1-5                            61,230
â”œâ”€Linear: 1-6                            65
=================================================================
Total params: 72,943
Trainable params: 72,943
Non-trainable params: 0
=================================================================
Logging to ./chess_logs/2_MaskablePPO_Baseline_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6682     |
|    invalid_moves   | 0        |
|    losses          | 3245     |
|    valid_moves     | 130000   |
|    win_rate        | 0.514    |
|    wins            | 3437     |
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 2434     |
|    iterations      | 1        |
|    time_elapsed    | 53       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 13505        |
|    invalid_moves        | 0            |
|    losses               | 6299         |
|    valid_moves          | 260000       |
|    win_rate             | 0.534        |
|    wins                 | 7206         |
| rollout/                |              |
|    ep_len_mean          | 18.6         |
|    ep_rew_mean          | 7.08         |
| time/                   |              |
|    fps                  | 2347         |
|    iterations           | 2            |
|    time_elapsed         | 111          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0020757983 |
|    clip_fraction        | 0.00551      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | -0.000792    |
|    learning_rate        | 0.0001       |
|    loss                 | 682          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00779     |
|    value_loss           | 1.45e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 20597       |
|    invalid_moves        | 0           |
|    losses               | 9049        |
|    valid_moves          | 390000      |
|    win_rate             | 0.561       |
|    wins                 | 11548       |
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 35.1        |
| time/                   |             |
|    fps                  | 2319        |
|    iterations           | 3           |
|    time_elapsed         | 169         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.002347868 |
|    clip_fraction        | 0.00775     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.00275     |
|    learning_rate        | 0.0001      |
|    loss                 | 755         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.47e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 28169       |
|    invalid_moves        | 0           |
|    losses               | 11546       |
|    valid_moves          | 520000      |
|    win_rate             | 0.59        |
|    wins                 | 16623       |
| rollout/                |             |
|    ep_len_mean          | 15.6        |
|    ep_rew_mean          | 37.2        |
| time/                   |             |
|    fps                  | 2300        |
|    iterations           | 4           |
|    time_elapsed         | 227         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.003314381 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.00467     |
|    learning_rate        | 0.0001      |
|    loss                 | 727         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 36254        |
|    invalid_moves        | 0            |
|    losses               | 13805        |
|    valid_moves          | 650000       |
|    win_rate             | 0.619        |
|    wins                 | 22449        |
| rollout/                |              |
|    ep_len_mean          | 14.4         |
|    ep_rew_mean          | 64.5         |
| time/                   |              |
|    fps                  | 2282         |
|    iterations           | 5            |
|    time_elapsed         | 287          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0016393939 |
|    clip_fraction        | 0.00779      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.00712      |
|    learning_rate        | 0.0001       |
|    loss                 | 884          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 1.62e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 44862        |
|    invalid_moves        | 0            |
|    losses               | 15974        |
|    valid_moves          | 780000       |
|    win_rate             | 0.644        |
|    wins                 | 28888        |
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 92.7         |
| time/                   |              |
|    fps                  | 2268         |
|    iterations           | 6            |
|    time_elapsed         | 346          |
|    total_timesteps      | 786432       |
| train/                  |              |
|    approx_kl            | 0.0012541063 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.00712      |
|    learning_rate        | 0.0001       |
|    loss                 | 903          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 1.72e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 54373        |
|    invalid_moves        | 0            |
|    losses               | 18109        |
|    valid_moves          | 910000       |
|    win_rate             | 0.667        |
|    wins                 | 36264        |
| rollout/                |              |
|    ep_len_mean          | 14.7         |
|    ep_rew_mean          | 106          |
| time/                   |              |
|    fps                  | 2251         |
|    iterations           | 7            |
|    time_elapsed         | 407          |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0018181889 |
|    clip_fraction        | 0.00756      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.00793      |
|    learning_rate        | 0.0001       |
|    loss                 | 899          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 1.81e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 64671        |
|    invalid_moves        | 0            |
|    losses               | 20065        |
|    valid_moves          | 1040000      |
|    win_rate             | 0.69         |
|    wins                 | 44606        |
| rollout/                |              |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 126          |
| time/                   |              |
|    fps                  | 2234         |
|    iterations           | 8            |
|    time_elapsed         | 469          |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0027711128 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.45        |
|    explained_variance   | 0.00857      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 1.98e+03     |
------------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/2_MaskablePPO_Baseline_20250804_133608.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 2_MaskablePPO_Baseline agent...
[0;36m-> Mean Reward: 160.90 +/- 1.09[0m
[0;37mParsed arguments:[0m
[0;37m   agent_name: 3_MaskableRecurrentPPO[0m
[0;37m   model_path: models/3_MaskableRecurrentPPO_20250804_134412.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '3_MaskableRecurrentPPO' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
MaskableRecurrentMultiInputActorCriticPolicy  --
â”œâ”€CombinedExtractor: 1-1                      --
â”‚    â””â”€ModuleDict: 2-1                        --
â”‚    â”‚    â””â”€Flatten: 3-1                      --
â”œâ”€CombinedExtractor: 1-2                      --
â”‚    â””â”€ModuleDict: 2-2                        --
â”‚    â”‚    â””â”€Flatten: 3-2                      --
â”œâ”€CombinedExtractor: 1-3                      --
â”‚    â””â”€ModuleDict: 2-3                        --
â”‚    â”‚    â””â”€Flatten: 3-3                      --
â”œâ”€MlpExtractor: 1-4                           --
â”‚    â””â”€Sequential: 2-4                        --
â”‚    â”‚    â””â”€Linear: 3-4                       16,448
â”‚    â”‚    â””â”€Tanh: 3-5                         --
â”‚    â”‚    â””â”€Linear: 3-6                       4,160
â”‚    â”‚    â””â”€Tanh: 3-7                         --
â”‚    â””â”€Sequential: 2-5                        --
â”‚    â”‚    â””â”€Linear: 3-8                       16,448
â”‚    â”‚    â””â”€Tanh: 3-9                         --
â”‚    â”‚    â””â”€Linear: 3-10                      4,160
â”‚    â”‚    â””â”€Tanh: 3-11                        --
â”œâ”€Linear: 1-5                                 61,230
â”œâ”€Linear: 1-6                                 65
â”œâ”€LSTM: 1-7                                   289,792
â”œâ”€LSTM: 1-8                                   289,792
======================================================================
Total params: 682,095
Trainable params: 682,095
Non-trainable params: 0
======================================================================
Logging to ./chess_logs/3_MaskableRecurrentPPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6728     |
|    invalid_moves   | 0        |
|    losses          | 3394     |
|    valid_moves     | 130000   |
|    win_rate        | 0.496    |
|    wins            | 3334     |
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | -0.741   |
| time/              |          |
|    fps             | 2069     |
|    iterations      | 1        |
|    time_elapsed    | 63       |
|    total_timesteps | 131072   |
---------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 13427         |
|    invalid_moves        | 0             |
|    losses               | 6598          |
|    valid_moves          | 260000        |
|    win_rate             | 0.509         |
|    wins                 | 6829          |
| rollout/                |               |
|    ep_len_mean          | 18.4          |
|    ep_rew_mean          | 14.9          |
| time/                   |               |
|    fps                  | 1392          |
|    iterations           | 2             |
|    time_elapsed         | 188           |
|    total_timesteps      | 262144        |
| train/                  |               |
|    approx_kl            | 0.00023304514 |
|    clip_fraction        | 7.93e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.59         |
|    explained_variance   | 0.000483      |
|    learning_rate        | 0.0001        |
|    loss                 | 712           |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.002        |
|    value_loss           | 1.45e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 20148         |
|    invalid_moves        | 0             |
|    losses               | 9794          |
|    valid_moves          | 390000        |
|    win_rate             | 0.514         |
|    wins                 | 10354         |
| rollout/                |               |
|    ep_len_mean          | 18.9          |
|    ep_rew_mean          | 0.555         |
| time/                   |               |
|    fps                  | 1255          |
|    iterations           | 3             |
|    time_elapsed         | 313           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00020693745 |
|    clip_fraction        | 9.61e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.58         |
|    explained_variance   | 0.00763       |
|    learning_rate        | 0.0001        |
|    loss                 | 745           |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00214      |
|    value_loss           | 1.44e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 26966         |
|    invalid_moves        | 0             |
|    losses               | 12843         |
|    valid_moves          | 520000        |
|    win_rate             | 0.524         |
|    wins                 | 14123         |
| rollout/                |               |
|    ep_len_mean          | 23.1          |
|    ep_rew_mean          | 15.4          |
| time/                   |               |
|    fps                  | 1196          |
|    iterations           | 4             |
|    time_elapsed         | 438           |
|    total_timesteps      | 524288        |
| train/                  |               |
|    approx_kl            | 0.00027127724 |
|    clip_fraction        | 0.00036       |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.58         |
|    explained_variance   | 0.0102        |
|    learning_rate        | 0.0001        |
|    loss                 | 719           |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.00267      |
|    value_loss           | 1.44e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 33837         |
|    invalid_moves        | 0             |
|    losses               | 15767         |
|    valid_moves          | 650000        |
|    win_rate             | 0.534         |
|    wins                 | 18070         |
| rollout/                |               |
|    ep_len_mean          | 18.1          |
|    ep_rew_mean          | 28.8          |
| time/                   |               |
|    fps                  | 1163          |
|    iterations           | 5             |
|    time_elapsed         | 563           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.00029850216 |
|    clip_fraction        | 0.000658      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.58         |
|    explained_variance   | 0.0109        |
|    learning_rate        | 0.0001        |
|    loss                 | 682           |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.00309      |
|    value_loss           | 1.46e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 40847         |
|    invalid_moves        | 0             |
|    losses               | 18627         |
|    valid_moves          | 780000        |
|    win_rate             | 0.544         |
|    wins                 | 22220         |
| rollout/                |               |
|    ep_len_mean          | 17            |
|    ep_rew_mean          | 38.1          |
| time/                   |               |
|    fps                  | 1140          |
|    iterations           | 6             |
|    time_elapsed         | 689           |
|    total_timesteps      | 786432        |
| train/                  |               |
|    approx_kl            | 0.00027691695 |
|    clip_fraction        | 0.00088       |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.57         |
|    explained_variance   | 0.0116        |
|    learning_rate        | 0.0001        |
|    loss                 | 750           |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.0035       |
|    value_loss           | 1.48e+03      |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 48096         |
|    invalid_moves        | 0             |
|    losses               | 21467         |
|    valid_moves          | 910000        |
|    win_rate             | 0.554         |
|    wins                 | 26629         |
| rollout/                |               |
|    ep_len_mean          | 18.3          |
|    ep_rew_mean          | -3.01         |
| time/                   |               |
|    fps                  | 1123          |
|    iterations           | 7             |
|    time_elapsed         | 816           |
|    total_timesteps      | 917504        |
| train/                  |               |
|    approx_kl            | 0.00074240135 |
|    clip_fraction        | 0.00162       |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.56         |
|    explained_variance   | 0.0111        |
|    learning_rate        | 0.0001        |
|    loss                 | 737           |
|    n_updates            | 60            |
|    policy_gradient_loss | -0.00483      |
|    value_loss           | 1.5e+03       |
-------------------------------------------
-------------------------------------------
| custom/                 |               |
|    draws                | 0             |
|    episodes             | 55543         |
|    invalid_moves        | 0             |
|    losses               | 24172         |
|    valid_moves          | 1040000       |
|    win_rate             | 0.565         |
|    wins                 | 31371         |
| rollout/                |               |
|    ep_len_mean          | 17            |
|    ep_rew_mean          | 48            |
| time/                   |               |
|    fps                  | 1110          |
|    iterations           | 8             |
|    time_elapsed         | 944           |
|    total_timesteps      | 1048576       |
| train/                  |               |
|    approx_kl            | 0.00051585864 |
|    clip_fraction        | 0.00212       |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.56         |
|    explained_variance   | 0.0117        |
|    learning_rate        | 0.0001        |
|    loss                 | 794           |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.00551      |
|    value_loss           | 1.55e+03      |
-------------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/3_MaskableRecurrentPPO_20250804_134412.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 3_MaskableRecurrentPPO agent...
[0;36m-> Mean Reward: 158.35 +/- 41.93[0m
[0;37mParsed arguments:[0m
[0;37m   n_samples: 1000000[0m
[0;37m   board_file_path: boards/boards.npy[0m
[0;37m   weights_file_path: weights/ae_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 4_FF_Autoencoder_MaskablePPO[0m
[0;37m   model_path: models/4_FF_Autoencoder_MaskablePPO_20250804_140110.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MaskableMultiInputActorCriticPolicy      --
â”œâ”€FrozenAEFeatureExtractor: 1-1          --
â”‚    â””â”€Sequential: 2-1                   --
â”‚    â”‚    â””â”€Flatten: 3-1                 --
â”‚    â”‚    â””â”€Linear: 3-2                  (1,664)
â”‚    â”‚    â””â”€ReLU: 3-3                    --
â”‚    â”‚    â””â”€Linear: 3-4                  (520)
â”œâ”€FrozenAEFeatureExtractor: 1-2          (recursive)
â”‚    â””â”€Sequential: 2-2                   (recursive)
â”‚    â”‚    â””â”€Flatten: 3-5                 --
â”‚    â”‚    â””â”€Linear: 3-6                  (recursive)
â”‚    â”‚    â””â”€ReLU: 3-7                    --
â”‚    â”‚    â””â”€Linear: 3-8                  (recursive)
â”œâ”€FrozenAEFeatureExtractor: 1-3          (recursive)
â”‚    â””â”€Sequential: 2-3                   (recursive)
â”‚    â”‚    â””â”€Flatten: 3-9                 --
â”‚    â”‚    â””â”€Linear: 3-10                 (recursive)
â”‚    â”‚    â””â”€ReLU: 3-11                   --
â”‚    â”‚    â””â”€Linear: 3-12                 (recursive)
â”œâ”€MlpExtractor: 1-4                      --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-13                 576
â”‚    â”‚    â””â”€Tanh: 3-14                   --
â”‚    â”‚    â””â”€Linear: 3-15                 4,160
â”‚    â”‚    â””â”€Tanh: 3-16                   --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-17                 576
â”‚    â”‚    â””â”€Tanh: 3-18                   --
â”‚    â”‚    â””â”€Linear: 3-19                 4,160
â”‚    â”‚    â””â”€Tanh: 3-20                   --
â”œâ”€Linear: 1-5                            61,230
â”œâ”€Linear: 1-6                            65
=================================================================
Total params: 72,951
Trainable params: 70,767
Non-trainable params: 2,184
=================================================================
Logging to ./chess_logs/4_FF_Autoencoder_MaskablePPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6641     |
|    invalid_moves   | 0        |
|    losses          | 3218     |
|    valid_moves     | 130000   |
|    win_rate        | 0.515    |
|    wins            | 3423     |
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 36.8     |
| time/              |          |
|    fps             | 2414     |
|    iterations      | 1        |
|    time_elapsed    | 54       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 13547        |
|    invalid_moves        | 0            |
|    losses               | 6058         |
|    valid_moves          | 260000       |
|    win_rate             | 0.553        |
|    wins                 | 7489         |
| rollout/                |              |
|    ep_len_mean          | 20.7         |
|    ep_rew_mean          | 47.3         |
| time/                   |              |
|    fps                  | 2329         |
|    iterations           | 2            |
|    time_elapsed         | 112          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0108144935 |
|    clip_fraction        | 0.0552       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | -2.5e-06     |
|    learning_rate        | 0.0001       |
|    loss                 | 704          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 1.44e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 20904       |
|    invalid_moves        | 0           |
|    losses               | 8570        |
|    valid_moves          | 390000      |
|    win_rate             | 0.59        |
|    wins                 | 12334       |
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 71.5        |
| time/                   |             |
|    fps                  | 2296        |
|    iterations           | 3           |
|    time_elapsed         | 171         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.011896118 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.000183    |
|    learning_rate        | 0.0001      |
|    loss                 | 830         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 28947       |
|    invalid_moves        | 0           |
|    losses               | 10725       |
|    valid_moves          | 520000      |
|    win_rate             | 0.629       |
|    wins                 | 18222       |
| rollout/                |             |
|    ep_len_mean          | 14.1        |
|    ep_rew_mean          | 102         |
| time/                   |             |
|    fps                  | 2274        |
|    iterations           | 4           |
|    time_elapsed         | 230         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.011894481 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.000172    |
|    learning_rate        | 0.0001      |
|    loss                 | 746         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 1.59e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 37889       |
|    invalid_moves        | 0           |
|    losses               | 12577       |
|    valid_moves          | 650000      |
|    win_rate             | 0.668       |
|    wins                 | 25312       |
| rollout/                |             |
|    ep_len_mean          | 13.4        |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 2251        |
|    iterations           | 5           |
|    time_elapsed         | 291         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.011242216 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.000193    |
|    learning_rate        | 0.0001      |
|    loss                 | 863         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 1.71e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 48084       |
|    invalid_moves        | 0           |
|    losses               | 14213       |
|    valid_moves          | 780000      |
|    win_rate             | 0.704       |
|    wins                 | 33871       |
| rollout/                |             |
|    ep_len_mean          | 11.4        |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 2227        |
|    iterations           | 6           |
|    time_elapsed         | 353         |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.010413231 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 6.84e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 931         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 1.86e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 59615       |
|    invalid_moves        | 0           |
|    losses               | 15644       |
|    valid_moves          | 910000      |
|    win_rate             | 0.738       |
|    wins                 | 43971       |
| rollout/                |             |
|    ep_len_mean          | 10.3        |
|    ep_rew_mean          | 124         |
| time/                   |             |
|    fps                  | 2203        |
|    iterations           | 7           |
|    time_elapsed         | 416         |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.009130151 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 4.18e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 999         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 2.09e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 72549       |
|    invalid_moves        | 0           |
|    losses               | 16895       |
|    valid_moves          | 1040000     |
|    win_rate             | 0.767       |
|    wins                 | 55654       |
| rollout/                |             |
|    ep_len_mean          | 11.4        |
|    ep_rew_mean          | 142         |
| time/                   |             |
|    fps                  | 2181        |
|    iterations           | 8           |
|    time_elapsed         | 480         |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.009122167 |
|    clip_fraction        | 0.0713      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 2.89e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.16e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 2.3e+03     |
-----------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/4_FF_Autoencoder_MaskablePPO_20250804_140110.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 4_FF_Autoencoder_MaskablePPO agent...
[0;36m-> Mean Reward: 145.05 +/- 71.46[0m
[0;37mParsed arguments:[0m
[0;37m   n_samples: 1000000[0m
[0;37m   board_file_path: boards/boards.npy[0m
[0;37m   weights_file_path: weights/ae_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 5_FF_Autoencoder_MaskableRecurrentPPO[0m
[0;37m   model_path: models/5_FF_Autoencoder_MaskableRecurrentPPO_20250804_140928.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '5_FF_Autoencoder_MaskableRecurrentPPO' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
MaskableRecurrentMultiInputActorCriticPolicy  --
â”œâ”€FrozenAEFeatureExtractor: 1-1               --
â”‚    â””â”€Sequential: 2-1                        --
â”‚    â”‚    â””â”€Flatten: 3-1                      --
â”‚    â”‚    â””â”€Linear: 3-2                       (1,664)
â”‚    â”‚    â””â”€ReLU: 3-3                         --
â”‚    â”‚    â””â”€Linear: 3-4                       (520)
â”œâ”€FrozenAEFeatureExtractor: 1-2               (recursive)
â”‚    â””â”€Sequential: 2-2                        (recursive)
â”‚    â”‚    â””â”€Flatten: 3-5                      --
â”‚    â”‚    â””â”€Linear: 3-6                       (recursive)
â”‚    â”‚    â””â”€ReLU: 3-7                         --
â”‚    â”‚    â””â”€Linear: 3-8                       (recursive)
â”œâ”€FrozenAEFeatureExtractor: 1-3               (recursive)
â”‚    â””â”€Sequential: 2-3                        (recursive)
â”‚    â”‚    â””â”€Flatten: 3-9                      --
â”‚    â”‚    â””â”€Linear: 3-10                      (recursive)
â”‚    â”‚    â””â”€ReLU: 3-11                        --
â”‚    â”‚    â””â”€Linear: 3-12                      (recursive)
â”œâ”€MlpExtractor: 1-4                           --
â”‚    â””â”€Sequential: 2-4                        --
â”‚    â”‚    â””â”€Linear: 3-13                      16,448
â”‚    â”‚    â””â”€Tanh: 3-14                        --
â”‚    â”‚    â””â”€Linear: 3-15                      4,160
â”‚    â”‚    â””â”€Tanh: 3-16                        --
â”‚    â””â”€Sequential: 2-5                        --
â”‚    â”‚    â””â”€Linear: 3-17                      16,448
â”‚    â”‚    â””â”€Tanh: 3-18                        --
â”‚    â”‚    â””â”€Linear: 3-19                      4,160
â”‚    â”‚    â””â”€Tanh: 3-20                        --
â”œâ”€Linear: 1-5                                 61,230
â”œâ”€Linear: 1-6                                 65
â”œâ”€LSTM: 1-7                                   272,384
â”œâ”€LSTM: 1-8                                   272,384
======================================================================
Total params: 649,463
Trainable params: 647,279
Non-trainable params: 2,184
======================================================================
Logging to ./chess_logs/5_FF_Autoencoder_MaskableRecurrentPPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6748     |
|    invalid_moves   | 0        |
|    losses          | 3348     |
|    valid_moves     | 130000   |
|    win_rate        | 0.504    |
|    wins            | 3400     |
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 2054     |
|    iterations      | 1        |
|    time_elapsed    | 63       |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 13719       |
|    invalid_moves        | 0           |
|    losses               | 6302        |
|    valid_moves          | 260000      |
|    win_rate             | 0.541       |
|    wins                 | 7417        |
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 20.5        |
| time/                   |             |
|    fps                  | 1385        |
|    iterations           | 2           |
|    time_elapsed         | 189         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.009776228 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 7.33e-06    |
|    learning_rate        | 0.0001      |
|    loss                 | 763         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 1.47e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 21164       |
|    invalid_moves        | 0           |
|    losses               | 8730        |
|    valid_moves          | 390000      |
|    win_rate             | 0.588       |
|    wins                 | 12434       |
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 63.9        |
| time/                   |             |
|    fps                  | 1244        |
|    iterations           | 3           |
|    time_elapsed         | 316         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.011680168 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.000689    |
|    learning_rate        | 0.0001      |
|    loss                 | 715         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 1.51e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 29331       |
|    invalid_moves        | 0           |
|    losses               | 10905       |
|    valid_moves          | 520000      |
|    win_rate             | 0.628       |
|    wins                 | 18426       |
| rollout/                |             |
|    ep_len_mean          | 15.9        |
|    ep_rew_mean          | 49.5        |
| time/                   |             |
|    fps                  | 1175        |
|    iterations           | 4           |
|    time_elapsed         | 446         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.010682862 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.00398     |
|    learning_rate        | 0.0001      |
|    loss                 | 727         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 1.59e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 38423       |
|    invalid_moves        | 0           |
|    losses               | 12823       |
|    valid_moves          | 650000      |
|    win_rate             | 0.666       |
|    wins                 | 25600       |
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 113         |
| time/                   |             |
|    fps                  | 1127        |
|    iterations           | 5           |
|    time_elapsed         | 581         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.009034207 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.00693     |
|    learning_rate        | 0.0001      |
|    loss                 | 871         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 1.72e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 48476       |
|    invalid_moves        | 0           |
|    losses               | 14433       |
|    valid_moves          | 780000      |
|    win_rate             | 0.702       |
|    wins                 | 34043       |
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 134         |
| time/                   |             |
|    fps                  | 1091        |
|    iterations           | 6           |
|    time_elapsed         | 720         |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.011297577 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.0118      |
|    learning_rate        | 0.0001      |
|    loss                 | 945         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 1.88e+03    |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 59858        |
|    invalid_moves        | 0            |
|    losses               | 15887        |
|    valid_moves          | 910000       |
|    win_rate             | 0.735        |
|    wins                 | 43971        |
| rollout/                |              |
|    ep_len_mean          | 10.7         |
|    ep_rew_mean          | 106          |
| time/                   |              |
|    fps                  | 1059         |
|    iterations           | 7            |
|    time_elapsed         | 866          |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0105494745 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.0149       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.06e+03     |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0264      |
|    value_loss           | 2.03e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 72900       |
|    invalid_moves        | 0           |
|    losses               | 17155       |
|    valid_moves          | 1040000     |
|    win_rate             | 0.765       |
|    wins                 | 55745       |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 124         |
| time/                   |             |
|    fps                  | 1030        |
|    iterations           | 8           |
|    time_elapsed         | 1017        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.010496529 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.0194      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 2.24e+03    |
-----------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/5_FF_Autoencoder_MaskableRecurrentPPO_20250804_140928.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 5_FF_Autoencoder_MaskableRecurrentPPO agent...
[0;36m-> Mean Reward: 151.65 +/- 63.02[0m
[0;37mParsed arguments:[0m
[0;37m   n_samples: 1000000[0m
[0;37m   board_file_path: boards/board_seqs.npy[0m
[0;37m   weights_file_path: weights/ae_rnn_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 6_LSTM_Autoencoder_MaskablePPO[0m
[0;37m   model_path: models/6_LSTM_Autoencoder_MaskablePPO_20250804_142804.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '6_LSTM_Autoencoder_MaskablePPO' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/DRL/Chess_6_LSTM_Autoencoder_MaskablePPO.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ae.load_state_dict(torch.load(AE_WEIGHTS_PATH, map_location=device))
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
MaskableMultiInputActorCriticPolicy           --
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-1        --
â”‚    â””â”€SimpleLSTMAutoencoder: 2-1             --
â”‚    â”‚    â””â”€LSTM: 3-1                         (23,296)
â”‚    â”‚    â””â”€Linear: 3-2                       (520)
â”‚    â”‚    â””â”€Linear: 3-3                       (576)
â”‚    â”‚    â””â”€LSTM: 3-4                         (33,280)
â”‚    â”‚    â””â”€Linear: 3-5                       (1,625)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-2             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-6                         (recursive)
â”‚    â”‚    â””â”€Linear: 3-7                       (recursive)
â”‚    â”‚    â””â”€Linear: 3-8                       (recursive)
â”‚    â”‚    â””â”€LSTM: 3-9                         (recursive)
â”‚    â”‚    â””â”€Linear: 3-10                      (recursive)
â”‚    â””â”€LSTM: 2-3                              (recursive)
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-2        (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-4             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-11                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-12                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-13                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-14                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-15                      (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-5             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-16                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-17                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-18                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-19                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-20                      (recursive)
â”‚    â””â”€LSTM: 2-6                              (recursive)
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-3        (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-7             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-21                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-22                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-23                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-24                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-25                      (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-8             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-26                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-27                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-28                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-29                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-30                      (recursive)
â”‚    â””â”€LSTM: 2-9                              (recursive)
â”œâ”€MlpExtractor: 1-4                           --
â”‚    â””â”€Sequential: 2-10                       --
â”‚    â”‚    â””â”€Linear: 3-31                      576
â”‚    â”‚    â””â”€Tanh: 3-32                        --
â”‚    â”‚    â””â”€Linear: 3-33                      4,160
â”‚    â”‚    â””â”€Tanh: 3-34                        --
â”‚    â””â”€Sequential: 2-11                       --
â”‚    â”‚    â””â”€Linear: 3-35                      576
â”‚    â”‚    â””â”€Tanh: 3-36                        --
â”‚    â”‚    â””â”€Linear: 3-37                      4,160
â”‚    â”‚    â””â”€Tanh: 3-38                        --
â”œâ”€Linear: 1-5                                 61,230
â”œâ”€Linear: 1-6                                 65
======================================================================
Total params: 130,064
Trainable params: 70,767
Non-trainable params: 59,297
======================================================================
Logging to ./chess_logs/6_LSTM_Autoencoder_MaskablePPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6671     |
|    invalid_moves   | 0        |
|    losses          | 3196     |
|    valid_moves     | 130000   |
|    win_rate        | 0.521    |
|    wins            | 3475     |
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | -5.1     |
| time/              |          |
|    fps             | 2316     |
|    iterations      | 1        |
|    time_elapsed    | 56       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 13572        |
|    invalid_moves        | 0            |
|    losses               | 6313         |
|    valid_moves          | 260000       |
|    win_rate             | 0.535        |
|    wins                 | 7259         |
| rollout/                |              |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 22.7         |
| time/                   |              |
|    fps                  | 2229         |
|    iterations           | 2            |
|    time_elapsed         | 117          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0029204716 |
|    clip_fraction        | 0.00242      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | -2.54e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 751          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00508     |
|    value_loss           | 1.45e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 20836       |
|    invalid_moves        | 0           |
|    losses               | 9023        |
|    valid_moves          | 390000      |
|    win_rate             | 0.567       |
|    wins                 | 11813       |
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 26.3        |
| time/                   |             |
|    fps                  | 2201        |
|    iterations           | 3           |
|    time_elapsed         | 178         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.009108679 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.000197    |
|    learning_rate        | 0.0001      |
|    loss                 | 712         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 28619        |
|    invalid_moves        | 0            |
|    losses               | 11408        |
|    valid_moves          | 520000       |
|    win_rate             | 0.601        |
|    wins                 | 17211        |
| rollout/                |              |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 84.7         |
| time/                   |              |
|    fps                  | 2182         |
|    iterations           | 4            |
|    time_elapsed         | 240          |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0064932257 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.54        |
|    explained_variance   | 0.000432     |
|    learning_rate        | 0.0001       |
|    loss                 | 737          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 1.57e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 37076       |
|    invalid_moves        | 0           |
|    losses               | 13514       |
|    valid_moves          | 650000      |
|    win_rate             | 0.636       |
|    wins                 | 23562       |
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 2168        |
|    iterations           | 5           |
|    time_elapsed         | 302         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.007234902 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.000614    |
|    learning_rate        | 0.0001      |
|    loss                 | 782         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 46707       |
|    invalid_moves        | 0           |
|    losses               | 15277       |
|    valid_moves          | 780000      |
|    win_rate             | 0.673       |
|    wins                 | 31430       |
| rollout/                |             |
|    ep_len_mean          | 13.4        |
|    ep_rew_mean          | 125         |
| time/                   |             |
|    fps                  | 2149        |
|    iterations           | 6           |
|    time_elapsed         | 365         |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.008911228 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.000703    |
|    learning_rate        | 0.0001      |
|    loss                 | 834         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 57625       |
|    invalid_moves        | 0           |
|    losses               | 16838       |
|    valid_moves          | 910000      |
|    win_rate             | 0.708       |
|    wins                 | 40787       |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 122         |
| time/                   |             |
|    fps                  | 2131        |
|    iterations           | 7           |
|    time_elapsed         | 430         |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.009177592 |
|    clip_fraction        | 0.0619      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.000751    |
|    learning_rate        | 0.0001      |
|    loss                 | 985         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 2e+03       |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 70038       |
|    invalid_moves        | 0           |
|    losses               | 18174       |
|    valid_moves          | 1040000     |
|    win_rate             | 0.741       |
|    wins                 | 51864       |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 142         |
| time/                   |             |
|    fps                  | 2112        |
|    iterations           | 8           |
|    time_elapsed         | 496         |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.010149834 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.000581    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.11e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 2.2e+03     |
-----------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/6_LSTM_Autoencoder_MaskablePPO_20250804_142804.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 6_LSTM_Autoencoder_MaskablePPO agent...
[0;36m-> Mean Reward: 154.84 +/- 42.34[0m
[0;37mParsed arguments:[0m
[0;37m   n_samples: 1000000[0m
[0;37m   board_file_path: boards/board_seqs.npy[0m
[0;37m   weights_file_path: weights/ae_rnn_pretrained.pth[0m
[0;37m   n_epochs: 20[0m
[0;37m   force_clean: False[0m
Skipping autoencoder training, weights already exists
[0;37mParsed arguments:[0m
[0;37m   agent_name: 7_LSTM_Autoencoder_MaskableRecurrentPPO[0m
[0;37m   model_path: models/7_LSTM_Autoencoder_MaskableRecurrentPPO_20250804_143638.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '7_LSTM_Autoencoder_MaskableRecurrentPPO' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/DRL/Chess_7_LSTM_Autoencoder_MaskableRecurrentPPO.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ae.load_state_dict(torch.load(AE_WEIGHTS_PATH, map_location=device))
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
MaskableRecurrentMultiInputActorCriticPolicy  --
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-1        --
â”‚    â””â”€SimpleLSTMAutoencoder: 2-1             --
â”‚    â”‚    â””â”€LSTM: 3-1                         (23,296)
â”‚    â”‚    â””â”€Linear: 3-2                       (520)
â”‚    â”‚    â””â”€Linear: 3-3                       (576)
â”‚    â”‚    â””â”€LSTM: 3-4                         (33,280)
â”‚    â”‚    â””â”€Linear: 3-5                       (1,625)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-2             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-6                         (recursive)
â”‚    â”‚    â””â”€Linear: 3-7                       (recursive)
â”‚    â”‚    â””â”€Linear: 3-8                       (recursive)
â”‚    â”‚    â””â”€LSTM: 3-9                         (recursive)
â”‚    â”‚    â””â”€Linear: 3-10                      (recursive)
â”‚    â””â”€LSTM: 2-3                              (recursive)
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-2        (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-4             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-11                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-12                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-13                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-14                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-15                      (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-5             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-16                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-17                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-18                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-19                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-20                      (recursive)
â”‚    â””â”€LSTM: 2-6                              (recursive)
â”œâ”€LSTMAutoencoderFeatureExtractor: 1-3        (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-7             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-21                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-22                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-23                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-24                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-25                      (recursive)
â”‚    â””â”€SimpleLSTMAutoencoder: 2-8             (recursive)
â”‚    â”‚    â””â”€LSTM: 3-26                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-27                      (recursive)
â”‚    â”‚    â””â”€Linear: 3-28                      (recursive)
â”‚    â”‚    â””â”€LSTM: 3-29                        (recursive)
â”‚    â”‚    â””â”€Linear: 3-30                      (recursive)
â”‚    â””â”€LSTM: 2-9                              (recursive)
â”œâ”€MlpExtractor: 1-4                           --
â”‚    â””â”€Sequential: 2-10                       --
â”‚    â”‚    â””â”€Linear: 3-31                      16,448
â”‚    â”‚    â””â”€Tanh: 3-32                        --
â”‚    â”‚    â””â”€Linear: 3-33                      4,160
â”‚    â”‚    â””â”€Tanh: 3-34                        --
â”‚    â””â”€Sequential: 2-11                       --
â”‚    â”‚    â””â”€Linear: 3-35                      16,448
â”‚    â”‚    â””â”€Tanh: 3-36                        --
â”‚    â”‚    â””â”€Linear: 3-37                      4,160
â”‚    â”‚    â””â”€Tanh: 3-38                        --
â”œâ”€Linear: 1-5                                 61,230
â”œâ”€Linear: 1-6                                 65
â”œâ”€LSTM: 1-7                                   272,384
â”œâ”€LSTM: 1-8                                   272,384
======================================================================
Total params: 706,576
Trainable params: 647,279
Non-trainable params: 59,297
======================================================================
Logging to ./chess_logs/7_LSTM_Autoencoder_MaskableRecurrentPPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6755     |
|    invalid_moves   | 0        |
|    losses          | 3295     |
|    valid_moves     | 130000   |
|    win_rate        | 0.512    |
|    wins            | 3460     |
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    fps             | 2011     |
|    iterations      | 1        |
|    time_elapsed    | 65       |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 13679       |
|    invalid_moves        | 0           |
|    losses               | 6233        |
|    valid_moves          | 260000      |
|    win_rate             | 0.544       |
|    wins                 | 7446        |
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 39.3        |
| time/                   |             |
|    fps                  | 1367        |
|    iterations           | 2           |
|    time_elapsed         | 191         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.006571149 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | -6.79e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 754         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 1.46e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 21143       |
|    invalid_moves        | 0           |
|    losses               | 8847        |
|    valid_moves          | 390000      |
|    win_rate             | 0.582       |
|    wins                 | 12296       |
| rollout/                |             |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 34.2        |
| time/                   |             |
|    fps                  | 1225        |
|    iterations           | 3           |
|    time_elapsed         | 320         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.012406732 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.56       |
|    explained_variance   | 0.000506    |
|    learning_rate        | 0.0001      |
|    loss                 | 720         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 1.5e+03     |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 29375       |
|    invalid_moves        | 0           |
|    losses               | 11110       |
|    valid_moves          | 520000      |
|    win_rate             | 0.622       |
|    wins                 | 18265       |
| rollout/                |             |
|    ep_len_mean          | 15.4        |
|    ep_rew_mean          | 91.5        |
| time/                   |             |
|    fps                  | 1157        |
|    iterations           | 4           |
|    time_elapsed         | 453         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.011845494 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.00191     |
|    learning_rate        | 0.0001      |
|    loss                 | 823         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 38329       |
|    invalid_moves        | 0           |
|    losses               | 13066       |
|    valid_moves          | 650000      |
|    win_rate             | 0.659       |
|    wins                 | 25263       |
| rollout/                |             |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | 81.7        |
| time/                   |             |
|    fps                  | 1113        |
|    iterations           | 5           |
|    time_elapsed         | 588         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.009748176 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.44       |
|    explained_variance   | 0.00645     |
|    learning_rate        | 0.0001      |
|    loss                 | 854         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 1.74e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 48378       |
|    invalid_moves        | 0           |
|    losses               | 14741       |
|    valid_moves          | 780000      |
|    win_rate             | 0.695       |
|    wins                 | 33637       |
| rollout/                |             |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | 101         |
| time/                   |             |
|    fps                  | 1078        |
|    iterations           | 6           |
|    time_elapsed         | 729         |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.009251358 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.0118      |
|    learning_rate        | 0.0001      |
|    loss                 | 895         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 1.85e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 59799       |
|    invalid_moves        | 0           |
|    losses               | 16238       |
|    valid_moves          | 910000      |
|    win_rate             | 0.728       |
|    wins                 | 43561       |
| rollout/                |             |
|    ep_len_mean          | 12.1        |
|    ep_rew_mean          | 133         |
| time/                   |             |
|    fps                  | 1049        |
|    iterations           | 7           |
|    time_elapsed         | 874         |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.009039842 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.0173      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.01e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 2.04e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 72594       |
|    invalid_moves        | 0           |
|    losses               | 17505       |
|    valid_moves          | 1040000     |
|    win_rate             | 0.759       |
|    wins                 | 55089       |
| rollout/                |             |
|    ep_len_mean          | 9.98        |
|    ep_rew_mean          | 144         |
| time/                   |             |
|    fps                  | 1020        |
|    iterations           | 8           |
|    time_elapsed         | 1027        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.008707198 |
|    clip_fraction        | 0.0743      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.0229      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/7_LSTM_Autoencoder_MaskableRecurrentPPO_20250804_143638.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 7_LSTM_Autoencoder_MaskableRecurrentPPO agent...
[0;36m-> Mean Reward: 157.28 +/- 44.59[0m
[0;37mParsed arguments:[0m
[0;37m   agent_name: 8_Naive_Transformer_PPO[0m
[0;37m   model_path: models/8_Naive_Transformer_PPO_20250804_145526.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '8_Naive_Transformer_PPO' agent using device 'cuda' and '16' parallel environments...
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead
  warnings.warn("constant_fn() is deprecated, please use ConstantSchedule() instead")
Using cuda device
[0;36m-> Model summary:[0m
=====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
MaskableMultiInputActorCriticPolicy                          --
â”œâ”€TransformerFeatureExtractor: 1-1                           --
â”‚    â””â”€TinyGPT2Encoder: 2-1                                  --
â”‚    â”‚    â””â”€GPT2Model: 3-1                                   108,096
â”‚    â””â”€Linear: 2-2                                           1,664
â”œâ”€TransformerFeatureExtractor: 1-2                           (recursive)
â”‚    â””â”€TinyGPT2Encoder: 2-3                                  (recursive)
â”‚    â”‚    â””â”€GPT2Model: 3-2                                   (recursive)
â”‚    â””â”€Linear: 2-4                                           (recursive)
â”œâ”€TransformerFeatureExtractor: 1-3                           (recursive)
â”‚    â””â”€TinyGPT2Encoder: 2-5                                  (recursive)
â”‚    â”‚    â””â”€GPT2Model: 3-3                                   (recursive)
â”‚    â””â”€Linear: 2-6                                           (recursive)
â”œâ”€MlpExtractor: 1-4                                          --
â”‚    â””â”€Sequential: 2-7                                       --
â”‚    â”‚    â””â”€Linear: 3-4                                      4,160
â”‚    â”‚    â””â”€Tanh: 3-5                                        --
â”‚    â”‚    â””â”€Linear: 3-6                                      4,160
â”‚    â”‚    â””â”€Tanh: 3-7                                        --
â”‚    â””â”€Sequential: 2-8                                       --
â”‚    â”‚    â””â”€Linear: 3-8                                      4,160
â”‚    â”‚    â””â”€Tanh: 3-9                                        --
â”‚    â”‚    â””â”€Linear: 3-10                                     4,160
â”‚    â”‚    â””â”€Tanh: 3-11                                       --
â”œâ”€Linear: 1-5                                                61,230
â”œâ”€Linear: 1-6                                                65
=====================================================================================
Total params: 187,695
Trainable params: 187,695
Non-trainable params: 0
=====================================================================================
Logging to ./chess_logs/8_Naive_Transformer_PPO_1
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 6647     |
|    invalid_moves   | 0        |
|    losses          | 3238     |
|    valid_moves     | 130000   |
|    win_rate        | 0.513    |
|    wins            | 3409     |
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 1796     |
|    iterations      | 1        |
|    time_elapsed    | 72       |
|    total_timesteps | 131072   |
---------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 13479        |
|    invalid_moves        | 0            |
|    losses               | 6145         |
|    valid_moves          | 260000       |
|    win_rate             | 0.544        |
|    wins                 | 7334         |
| rollout/                |              |
|    ep_len_mean          | 18.1         |
|    ep_rew_mean          | -3.07        |
| time/                   |              |
|    fps                  | 1721         |
|    iterations           | 2            |
|    time_elapsed         | 152          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0036756548 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.59        |
|    explained_variance   | 0.000261     |
|    learning_rate        | 0.0001       |
|    loss                 | 688          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 1.44e+03     |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 20938        |
|    invalid_moves        | 0            |
|    losses               | 8702         |
|    valid_moves          | 390000       |
|    win_rate             | 0.584        |
|    wins                 | 12236        |
| rollout/                |              |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | 23.1         |
| time/                   |              |
|    fps                  | 1696         |
|    iterations           | 3            |
|    time_elapsed         | 231          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0046913833 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.57        |
|    explained_variance   | 0.00727      |
|    learning_rate        | 0.0001       |
|    loss                 | 741          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0131      |
|    value_loss           | 1.47e+03     |
------------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 29229       |
|    invalid_moves        | 0           |
|    losses               | 10967       |
|    valid_moves          | 520000      |
|    win_rate             | 0.625       |
|    wins                 | 18262       |
| rollout/                |             |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | 91.3        |
| time/                   |             |
|    fps                  | 1677        |
|    iterations           | 4           |
|    time_elapsed         | 312         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.006830902 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 0.0109      |
|    learning_rate        | 0.0001      |
|    loss                 | 829         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 1.59e+03    |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 38842       |
|    invalid_moves        | 0           |
|    losses               | 12845       |
|    valid_moves          | 650000      |
|    win_rate             | 0.669       |
|    wins                 | 25997       |
| rollout/                |             |
|    ep_len_mean          | 14.1        |
|    ep_rew_mean          | 96.3        |
| time/                   |             |
|    fps                  | 1660        |
|    iterations           | 5           |
|    time_elapsed         | 394         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.007494081 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.0118      |
|    learning_rate        | 0.0001      |
|    loss                 | 894         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 1.75e+03    |
-----------------------------------------
----------------------------------------
| custom/                 |            |
|    draws                | 0          |
|    episodes             | 50338      |
|    invalid_moves        | 0          |
|    losses               | 14331      |
|    valid_moves          | 780000     |
|    win_rate             | 0.715      |
|    wins                 | 36007      |
| rollout/                |            |
|    ep_len_mean          | 10.2       |
|    ep_rew_mean          | 139        |
| time/                   |            |
|    fps                  | 1642       |
|    iterations           | 6          |
|    time_elapsed         | 478        |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.00860971 |
|    clip_fraction        | 0.072      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.4       |
|    explained_variance   | 0.0111     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.01e+03   |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 1.98e+03   |
----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 64226       |
|    invalid_moves        | 0           |
|    losses               | 15565       |
|    valid_moves          | 910000      |
|    win_rate             | 0.758       |
|    wins                 | 48661       |
| rollout/                |             |
|    ep_len_mean          | 9.95        |
|    ep_rew_mean          | 120         |
| time/                   |             |
|    fps                  | 1624        |
|    iterations           | 7           |
|    time_elapsed         | 564         |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.009438263 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.00674     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 2.3e+03     |
-----------------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 80568       |
|    invalid_moves        | 0           |
|    losses               | 16668       |
|    valid_moves          | 1040000     |
|    win_rate             | 0.793       |
|    wins                 | 63900       |
| rollout/                |             |
|    ep_len_mean          | 7.13        |
|    ep_rew_mean          | 156         |
| time/                   |             |
|    fps                  | 1605        |
|    iterations           | 8           |
|    time_elapsed         | 652         |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.010901473 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.00521     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.35e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 2.69e+03    |
-----------------------------------------
/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/site-packages/stable_baselines3/common/utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead
  warnings.warn("get_schedule_fn() is deprecated, please use FloatSchedule() instead")
[0;32m-> Model saved on models/8_Naive_Transformer_PPO_20250804_145526.zip[0m
Initializing MiniChessEnv: 5x5 board with invalid action masking =True, original_step=False
Evaluating 8_Naive_Transformer_PPO agent...
[0;36m-> Mean Reward: 161.28 +/- 1.05[0m
[0;37mParsed arguments:[0m
[0;37m   agent_name: 1_PPO[0m
[0;37m   model_path: models/1_PPO_20250804_150639.zip[0m
[0;37m   log_dir: ./chess_logs[0m
[0;37m   total_timesteps: 1000000[0m
[0;37m   n_envs: 16[0m
[0;37m   n_steps: 8192[0m
[0;37m   batch_size: 8192[0m
[0;37m   n_epochs: 10[0m
[0;37m   n_samples_ae: 1000000[0m
[0;37m   n_epochs_ae: 20[0m
[0;37m   force_clean_ae: False[0m
Training '1_PPO' agent using device 'cuda' and '16' parallel environments...
Using cuda device
[0;36m-> Model summary:[0m
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MultiInputActorCriticPolicy              --
â”œâ”€CombinedExtractor: 1-1                 --
â”‚    â””â”€ModuleDict: 2-1                   --
â”‚    â”‚    â””â”€Flatten: 3-1                 --
â”‚    â”‚    â””â”€Flatten: 3-2                 --
â”œâ”€CombinedExtractor: 1-2                 --
â”‚    â””â”€ModuleDict: 2-2                   --
â”‚    â”‚    â””â”€Flatten: 3-3                 --
â”‚    â”‚    â””â”€Flatten: 3-4                 --
â”œâ”€CombinedExtractor: 1-3                 --
â”‚    â””â”€ModuleDict: 2-3                   --
â”‚    â”‚    â””â”€Flatten: 3-5                 --
â”‚    â”‚    â””â”€Flatten: 3-6                 --
â”œâ”€MlpExtractor: 1-4                      --
â”‚    â””â”€Sequential: 2-4                   --
â”‚    â”‚    â””â”€Linear: 3-7                  61,952
â”‚    â”‚    â””â”€Tanh: 3-8                    --
â”‚    â”‚    â””â”€Linear: 3-9                  4,160
â”‚    â”‚    â””â”€Tanh: 3-10                   --
â”‚    â””â”€Sequential: 2-5                   --
â”‚    â”‚    â””â”€Linear: 3-11                 61,952
â”‚    â”‚    â””â”€Tanh: 3-12                   --
â”‚    â”‚    â””â”€Linear: 3-13                 4,160
â”‚    â”‚    â””â”€Tanh: 3-14                   --
â”œâ”€Linear: 1-5                            61,230
â”œâ”€Linear: 1-6                            65
=================================================================
Total params: 193,519
Trainable params: 193,519
Non-trainable params: 0
=================================================================
Logging to ./chess_logs/1_PPO_2
---------------------------------
| custom/            |          |
|    draws           | 0        |
|    episodes        | 1        |
|    invalid_moves   | 128870   |
|    losses          | 1        |
|    valid_moves     | 1130     |
|    win_rate        | 0        |
|    wins            | 0        |
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -9.99    |
| time/              |          |
|    fps             | 5972     |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| custom/                 |             |
|    draws                | 0           |
|    episodes             | 4           |
|    invalid_moves        | 257602      |
|    losses               | 4           |
|    valid_moves          | 2398        |
|    win_rate             | 0           |
|    wins                 | 0           |
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -9.97       |
| time/                   |             |
|    fps                  | 5517        |
|    iterations           | 2           |
|    time_elapsed         | 47          |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.008063198 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | -0.0364     |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0789     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00554    |
|    value_loss           | 0.257       |
-----------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 6            |
|    invalid_moves        | 386121       |
|    losses               | 6            |
|    valid_moves          | 3879         |
|    win_rate             | 0            |
|    wins                 | 0            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.94        |
| time/                   |              |
|    fps                  | 5391         |
|    iterations           | 3            |
|    time_elapsed         | 72           |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0022012668 |
|    clip_fraction        | 0.00951      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.84        |
|    explained_variance   | -0.0048      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0674       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0037      |
|    value_loss           | 0.645        |
------------------------------------------
------------------------------------------
| custom/                 |              |
|    draws                | 0            |
|    episodes             | 10           |
|    invalid_moves        | 514412       |
|    losses               | 9            |
|    valid_moves          | 5588         |
|    win_rate             | 0.1          |
|    wins                 | 1            |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -9.92        |
| time/                   |              |
|    fps                  | 5312         |
|    iterations           | 4            |
|    time_elapsed         | 98           |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0034160647 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | -0.00105     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.5          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00498     |
|    value_loss           | 0.428        |
------------------------------------------
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
Initializing MiniChessEnv: 5x5 board with invalid action masking =False, original_step=False
[0;36m-> Starting experiments...[0m
[0;37mParsed arguments:[0m
[0;37m  parallel: False[0m
[0;37m  max_workers: 3[0m
[0;37m  num_repeats: 5[0m
[0;37mArguments to be forwarded: ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;36m-> Running Preprocessing Tasks...[0m
[0;35m-> Running script: ae_pretrain.py with args ['--n-samples', '100000', '--n-epochs', '20', '--force-clean', 'False'][0m
[0;32m-> [ae_pretrain.py] Finished in 2.9s[0m
[0;35m-> Running script: ae_rnn_pretrain.py with args ['--n-samples', '100000', '--n-epochs', '20', '--force-clean', 'False'][0m
[0;32m-> [ae_rnn_pretrain.py] Finished in 2.9s[0m
[0;35m-> Running experiments, num_repeats: 5, mode: Sequential[0m
[0;36m-> Iteration: 1[0m
[0;35m-> Running script: Chess_1_PPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_1_PPO.py] Finished in 223.8s[0m
[0;35m-> Running script: Chess_2_MaskablePPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_2_MaskablePPO.py] Finished in 483.3s[0m
[0;35m-> Running script: Chess_3_MaskableRecurrentPPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_3_MaskableRecurrentPPO.py] Finished in 1018.5s[0m
[0;35m-> Running script: Chess_4_FF_AutoEncoder_MaskablePPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_4_FF_AutoEncoder_MaskablePPO.py] Finished in 498.1s[0m
[0;35m-> Running script: Chess_5_FF_Autoencoder_MaskableRecurrentPPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_5_FF_Autoencoder_MaskableRecurrentPPO.py] Finished in 1115.7s[0m
[0;35m-> Running script: Chess_6_LSTM_Autoencoder_MaskablePPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_6_LSTM_Autoencoder_MaskablePPO.py] Finished in 514.2s[0m
[0;35m-> Running script: Chess_7_LSTM_Autoencoder_MaskableRecurrentPPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_7_LSTM_Autoencoder_MaskableRecurrentPPO.py] Finished in 1126.0s[0m
[0;35m-> Running script: Chess_8_Transformer.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
[0;32m-> [Chess_8_Transformer.py] Finished in 674.4s[0m
[0;36m-> Iteration: 2[0m
[0;35m-> Running script: Chess_1_PPO.py with args ['--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192'][0m
Traceback (most recent call last):
  File "/home/herreramaxi/DRL/train.py", line 125, in <module>
    run_experiments(args.num_repeats, args.parallel == "True")    
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/herreramaxi/DRL/train.py", line 80, in run_experiments
    run_exp(exp, unknown)
  File "/home/herreramaxi/DRL/train.py", line 93, in run_exp
    subprocess.run(cmd, check=True)
  File "/home/herreramaxi/miniconda3/envs/gymnasium-env/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['python', 'Chess_1_PPO.py', '--total-timesteps', '1000000', '--n-envs', '16', '--batch-size', '8192', '--n-steps', '8192']' died with <Signals.SIGTERM: 15>.
